{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "In this notebook, you're going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate.\n",
    "\n",
    "You'll then do some exploration of the generated images to visualize what the noise and class vectors mean.  \n",
    "\n",
    "### Learning Objectives\n",
    "1.   Learn the technical difference between a conditional and unconditional GAN.\n",
    "2.   Understand the distinction between the class and noise vector in a conditional GAN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "In this notebook, you're going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate.\n",
    "\n",
    "You'll then do some exploration of the generated images to visualize what the noise and class vectors mean.  \n",
    "\n",
    "### Learning Objectives\n",
    "1.   Learn the technical difference between a conditional and unconditional GAN.\n",
    "2.   Understand the distinction between the class and noise vector in a conditional GAN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbarbaric/dev/ai_projects/Generative-Adversarial-Networks-GANs-Specialization/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f247b7f5f30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor,\n",
    "                       num_images=25,\n",
    "                       size=(1, 28, 28),\n",
    "                       nrow=5,\n",
    "                       show=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        image_tensor (_type_): _description_\n",
    "        num_images (int, optional): _description_. Defaults to 25.\n",
    "        size (tuple, optional): _description_. Defaults to (1, 28, 28).\n",
    "        nrow (int, optional): _description_. Defaults to 5.\n",
    "        show (bool, optional): _description_. Defaults to True.\n",
    "    \"\"\"\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    \n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self._input_dim = input_dim\n",
    "        self._generator = nn.Sequential(\n",
    "            self._make_gen_block(input_dim, hidden_dim * 4),\n",
    "            self._make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self._make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self._make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True)\n",
    "        )\n",
    "\n",
    "    def _make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "    \n",
    "    def call(self, noise):\n",
    "        x = noise.view(len(noise), self._input_dim, 1, 1)\n",
    "        return self._generator(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    return torch.randn(n_samples, input_dim, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self._discriminator = nn.Sequential(\n",
    "            self._make_disc_block(im_chan, hidden_dim),\n",
    "            self._make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self._make_disc_block(hidden_dim * 2, 1, final_layer=True)\n",
    "        )\n",
    "\n",
    "    def _make_disc_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        if not final_layer:\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        disc_pred = self._discriminator(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Input\n",
    "\n",
    "In conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0's and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes, please create one-hot vectors for each label. There is a class within the PyTorch functional library that can help you.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Optional hints for <code><font size=\"4\">get_one_hot_labels</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1.   This code can be done in one line.\n",
    "2.   The documentation for [F.one_hot](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.one_hot) may be helpful.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 5)\n",
    "F.one_hot(torch.arange(0, 5), num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_labels(labels, n_classes):\n",
    "    return F.one_hot(labels, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert (\n",
    "    get_one_hot_labels(\n",
    "        labels=torch.Tensor([[0, 2, 1]]).long(),\n",
    "        n_classes=3\n",
    "    ).tolist() == \n",
    "    [[\n",
    "      [1, 0, 0], \n",
    "      [0, 0, 1], \n",
    "      [0, 1, 0]\n",
    "    ]]\n",
    ")\n",
    "# Check that the device of get_one_hot_labels matches the input device\n",
    "if torch.cuda.is_available():\n",
    "    assert str(get_one_hot_labels(torch.Tensor([[0]]).long().cuda(), 1).device).startswith(\"cuda\")\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator.\n",
    "\n",
    "To do this, you will need to write a function that combines two vectors. Remember that you need to ensure that the vectors are the same type: floats. Again, you can look to the PyTorch library for help.\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Optional hints for <code><font size=\"4\">combine_vectors</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1.   This code can also be written in one line.\n",
    "2.   The documentation for [torch.cat](https://pytorch.org/docs/master/generated/torch.cat.html) may be helpful.\n",
    "3.   Specifically, you might want to look at what the `dim` argument of `torch.cat` does.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.1704,  0.3980,  2.0654],\n",
       "         [ 0.0083,  1.2887,  0.5577]]),\n",
       " tensor([[-0.3457,  0.2602, -0.3509],\n",
       "         [ 0.6176, -1.4976,  0.8224]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.randn(2, 3)\n",
    "x2 = torch.randn(2, 3)\n",
    "x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1704,  0.3980,  2.0654, -0.3457,  0.2602, -0.3509],\n",
       "        [ 0.0083,  1.2887,  0.5577,  0.6176, -1.4976,  0.8224]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.cat([x1, x2], axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1704,  0.3980,  2.0654],\n",
       "        [ 0.0083,  1.2887,  0.5577],\n",
       "        [-0.3457,  0.2602, -0.3509],\n",
       "        [ 0.6176, -1.4976,  0.8224]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = torch.cat([x1, x2], axis=0)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_vectors(x, y):\n",
    "    return torch.cat([x.float(), y.float()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "combined = combine_vectors(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))\n",
    "if torch.cuda.is_available():\n",
    "    # Check that it doesn't break with cuda\n",
    "    cuda_check = combine_vectors(torch.tensor([[1, 2], [3, 4]]).cuda(), torch.tensor([[5, 6], [7, 8]]).cuda())\n",
    "    assert str(cuda_check.device).startswith(\"cuda\")\n",
    "# Check exact order of elements\n",
    "assert torch.all(combined == torch.tensor([[1, 2, 5, 6], [3, 4, 7, 8]]))\n",
    "# Tests that items are of float type\n",
    "assert (type(combined[0][0].item()) == float)\n",
    "# Check shapes\n",
    "combined = combine_vectors(torch.randn(1, 4, 5), torch.randn(1, 8, 5));\n",
    "assert tuple(combined.shape) == (1, 12, 5)\n",
    "assert tuple(combine_vectors(torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12).long()).shape) == (1, 30, 12)\n",
    "# Check that the float transformation doesn't happen after the inputs are concatenated\n",
    "assert tuple(combine_vectors(torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12)).shape) == (1, 30, 12)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now you can start to put it all together!\n",
    "First, you will define some new parameters:\n",
    "\n",
    "*   mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it's black-and-white) so 1 x 28 x 28\n",
    "*   n_classes: the number of classes in MNIST (10, since there are the digits from 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_shape = (1, 28, 28)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you also include the same parameters from previous assignments:\n",
    "\n",
    "  *   criterion: the loss function\n",
    "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
    "  *   z_dim: the dimension of the noise vector\n",
    "  *   display_step: how often to display/visualize the images\n",
    "  *   batch_size: the number of images per forward/backward pass\n",
    "  *   lr: the learning rate\n",
    "  *   device: the device type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))   \n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST('.', download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models. For the generator, you will need to calculate the size of the input vector; recall that for conditional GANs, the generator's input is the noise vector concatenated with the class vector. For the discriminator, you need to add a channel for every class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dimensions(z_dim, mnist_shape, n_classes):\n",
    "    '''\n",
    "    Function for getting the size of the conditional input dimensions \n",
    "    from z_dim, the image shape, and number of classes.\n",
    "    Parameters:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "                (10 for MNIST)\n",
    "    Returns: \n",
    "        generator_input_dim: the input dimensionality of the conditional generator, \n",
    "                          which takes the noise and class vectors\n",
    "        discriminator_im_chan: the number of input channels to the discriminator\n",
    "                            (e.g. C x 28 x 28 for MNIST)\n",
    "    '''\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "    discriminator_input_channel = mnist_shape[0]\n",
    "    return (generator_input_dim, discriminator_input_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "one, two = get_input_dimensions(z_dim, mnist_shape, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Conv2D'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m gen \u001b[38;5;241m=\u001b[39m Generator(input_dim\u001b[38;5;241m=\u001b[39mgenerator_input_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m gen_opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(gen\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m----> 5\u001b[0m disc \u001b[38;5;241m=\u001b[39m \u001b[43mDiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_chan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator_im_chan\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m disc_opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(disc\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweights_init\u001b[39m(m):\n",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36mDiscriminator.__init__\u001b[0;34m(self, im_chan, hidden_dim)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, im_chan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Discriminator, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discriminator \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_disc_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_chan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_disc_block(hidden_dim, hidden_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_disc_block(hidden_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, final_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m, in \u001b[0;36mDiscriminator._make_disc_block\u001b[0;34m(self, input_channels, output_channels, kernel_size, stride, final_layer)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_disc_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_channels, output_channels, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, final_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m final_layer:\n\u001b[1;32m     12\u001b[0m         nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 13\u001b[0m             \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m(input_channels, output_channels, kernel_size, stride),\n\u001b[1;32m     14\u001b[0m             nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(output_channels),\n\u001b[1;32m     15\u001b[0m             nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.2\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     19\u001b[0m             nn\u001b[38;5;241m.\u001b[39mConv2d(input_channels, output_channels, kernel_size, stride)\n\u001b[1;32m     20\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Conv2D'"
     ]
    }
   ],
   "source": [
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
